---
title: 服务架构
author: Yahui
layout: php
category: PHP
---


书名：《-》

<pre style="text-align: left;">
swoole
	多端口监听(order.php)
		protect $server
		protect $rpc
		function __construct($ip, $port) {
			$this->server = new Swoole\Http\Server($ip, $port);
			$this->server->on('request', [$this, 'onRequest']);
			$this->rpc = $this->server->addlistener('127.0.0.1', 9505, SWOOLE_SOCK_TCP);
			$this->rpc->set(['worker_num'=>1]);
			$this->rpc->on('receive',[$this, 'onReceive'])
		}
		public function onRequest($request,$response)
		{
			$goodsInfo = $this->csend('127.0.0.1', 9303, json_encode([
				....
			]))
			$response->end(goodsInfo);
		}
		public function onReceive($server,$fd,$reactorId,$data)
		{
			$server->send($fd, "aaaaaaa");
		}
		public function csend($ip,$port,$data)
		{
			$client = new Swoole\Corutine\Client(SWOOLE_SOCK_TCP);
			if (!$client->connect($ip,$port,0.5))
			{
				echo "连接失败";
			}
			$client->send($data);
			$return=$client->recv();
			$client->close();
			return $return;
		}
nginx使用upstream模块配置负载均衡(名字叫swoft_nginx)(在location中配置反向代理到负载均衡的 swoft_nginx)
	upstream soft_server {
		... // 负载均衡
	}
	server {
		...
		location / {
			proxy_pass http://soft_server
		}
	}
配置服务发现
	nginx配置文件upstream模块使用upsync加载consul
	upstream swoole_consul {
		server 192.168.169.140:9001; #留一个固定服务否则nginx启动报错
		upsync 127.0.0.1:8500/v1/kv/upstreams/swoole_test upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=on;
		upsync_dump_path /redis_2004/17/conf/servers_test.conf; #从consul读取的信息生成配置文件
		include /redis_2004/17/conf/servers_test.conf; #引入这个配置文件
	}
	解释：
		127.0.0.1:8500/v1/kv/upstreams -> 连接consul的api资源地址
		swoole_test -> 相当于我们自己在consul中自定义的key
		upsync_timeout -> 超时时间6分钟
		upsync_interval -> 定时获取信息的时间
		upsync_type -> 类型
		strong_dependency=on; -> 是否依赖consul运行
		upsync_dump_path -> 拉取之后申请配置文件
在nginx中配置好consul后,向consul增加swoft的集群地址,这样nginx就可以动态加载swoft
总结:
	nginx使用upsync动态获取consul中的服务信息, strong_dependency是否依赖, 获取服务信息后, 生成一份配置文件(比如swoft_server.conf)
	1.构建3swoft,1nginx,1consul
	2.配置nginx upstream 异步访问consul服务端
	3.consul:实际在程序自动注册信息到consul (当时使用的是curl)
docker-compose配置consul集群(部分代码)
	# 编排php,redis,nginx容器
	version: "3.6" # 确定docker-composer文件的版本
	services: # 代表就是一组服务 - 简单来说一组容器
	  # server
	  consul_master_server_170_30: # 这个表示服务的名称，自定义; 注意不是容器名称
	    image: consul1.4 # 指定容器的镜像文件
	    ports: # 配置容器与宿主机的端口
	      - "8500:8500"
	    networks: # 引入外部预先定义的网段
	       consul:
	         ipv4_address: 170.200.7.30   #设置ip地址
	    container_name: consul_master_server_170_30 # 这是容器的名称
	    volumes: #配置数据挂载
	      - 挂载目录(测试可以指定同一目录(因为在同一服务器上, 集群环境可以使用git+Jenkins), 实现代码同步)
	    command: ./consul agent -server -bootstrap-expect 3 -data-dir /tmp/consul -node=consul_master_server_170_30 -bind=170.200.7.30 -ui -client=0.0.0.0
	    depends_on: # 容器启动依赖的顺序
	    	- mongod_shard_A_174_2
	    注：
	    	-server // 代表是一个服务
	    	-bootstrap-expect 3 // 只在master节点，表示启动多少个节点（可以没有）
	    	-data-dir /tmp/consul
	    	-node=consul_master_server_170_30 // 节点名称
	    	-bind=170.200.7.30 // 绑定网络的通信方式，有IP或者端口映射
	    	-ui // consul的web界面
	    	-client=0.0.0.0
	    	-join=170.200.7.30  // 绑定到哪个节点，从节点（slave）会有
swoft
	可以利用注册与停止时间增加服务发现事件(官方提供的有方法(注:参数是服务的名称)),来实现swoft的注册与删除
docker-compose配置多个服务的问题
	问题:代码同步指定同一目录(swoft), 而swoft的配置文件是同一个,导致服务发现只能发现一个
	处理:使用服务器启动时的ip地址来配置.env文件,swoft启动读取.env文件来配置,从而实现同一套代码配置文件是多套
在swoft的listener下的RegisterServiceListener中,配置check模式
	"check" => [
		"name" => "swoft.goods.server" // 这个主要是区分与其他
		"tcp" => "你的swoft宿主机地址" // 这个如果是http，可以写成http的地址(http://test.com)
		"interval" => "10s" // 如果及时性比较低,可以设置长一些,这样对服务器压力小一点
		"timeout" => "2s" // 设置超时时间
	] 这个是用到swoft-consul组件, 这样consul就会生成一个定时器,检测swoft的健康状况
	总之,利用docker-composer,增加启动命令,给shell脚本传递IP,端口,shell脚本处理传递过来的参数,进行拼接,最终写进.env文件中,(形如HOST=12.12.12.11....),swoft启动读取.env文件内容获取IP及端口(例如第6点,将swoft地址拼接到swoft-consul组件配置项中),最终达到consul定时检测通过docker-composer启动的swoft服务
服务发现整体流程
	1.swoft调用.sh脚本
		1.启动客户端swoft时增加扩展命令
			php bin/swoft rpc:start ext_init=-ip:192.168....?-t:tcp?-p:18317....
		2.在swoft入口文件
			1.接收启动时的命令并处理形如 -h 192.168.... -t tcp -p 18317....
			2.执行脚本文件(并将上述处理后的命令拼接在执行脚本文件的命令后)
		3.init.sh脚本文件
			接收执行时的参数并获取各个参数的值写入env文件
		4.swoft监听事件
			注册事件,将服务写入consul中
				$this->kv->put('/upstream/swoft_server/' . env('IP') . ':' . env('PORT'), '{"max_fails":2,"fail_timeout":10}');
			删除事件,将服务从consul中删除
				$this->kv->delete('/upstream/swoft_server/' . env('IP') . ':' . env('PORT'));
	2..sh脚本执行swoft
		1.使用init.sh+扩展命令的形式启动脚本并
		2.将参数处理写入env文件
		3.init.sh脚本启动swoft
		4.swoft入口文件加载env文件
		5.注册/删除监听事件使用env内容
	注(还是2方便一些)
		swoft入口文件需要修改增加$console->setfile(好像是这个名字)('/var/www/env')来加载env文件
rabbitMQ+HA_proxy+keepalived高可用集群
	启动:systemctl start haproxy
	开机启动:systemctl enable haproxy
	haproxy:
		1.下载安装:yum install -y haproxy
		2.配置:/etc/haproxy/haproxy.cfg
			backend kubernetes-apiserver
				mode		tcp
				balance 	roundrobin // 轮询
				server 		master01.k8s.io IP:端口(k8s端口) check
				server 		master01.k8s.io IP:端口(k8s端口) check
		3.启动(指定配置文件形式)
			haproxy -f /etc/haproxy/haproxy.cfg
		4.整体流程
			配置文件haproxy.cfg
				HAProxy 的配置文件haproxy.cfg由两大部分组成，分别是global和proxies部分，配置文件对缩进没有要求
				-   global：全局配置段
				    -   进程及安全配置相关的参数 
				    -   性能调整相关参数 
				    -   Debug参数
				    例如:
					    log 127.0.0.1 local{1-7} info #基于syslog记录日志到指定设备，级别有(err、warning、info、debug)
						listen web_port
						  bind 127.0.0.1:80
						  mode http
						  log global #开启当前web_port的日志功能，默认不记录日志
						  server web1  127.0.0.1:8080 check inter 3000 fall 2 rise 5
				-   proxies：代理配置段
				    -   defaults：为frontend, backend, listen提供默认配置
				    	例如:
					    	option redispatch      #当server Id对应的服务器挂掉后，强制定向到其他健康的服务器，重新派发
							option abortonclose    #当服务器负载很高时，自动结束掉当前队列处理比较久的连接，针对业务情况选择开启，生产中一般不加
							option http-keep-alive #开启与客户端的会话保持
							option forwardfor      #透传客户端真实IP至后端web服务器
							mode http|tcp 		   #设置默认工作类型,后面listen的优先比比默认高，可以单独设置
							timeout http-keep-alive 120s  #session 会话保持超时时间，此时间段内会转发到相同的后端服务器
							timeout connect 120s   #客户端请求从haproxy到后端server最长连接等待时间(TCP连接之前)，默认单位ms
							timeout server 600s    #客户端请求从haproxy到后端服务端的请求处理超时时长(TCP连接之后)，默认单位ms，如果超时，会出现502错误，此值建议设置较大些，访止502错误
							timeout client 600s    #设置haproxy与客户端的最长非活动时间，默认单位ms，建议和timeout server相同
							timeout check   2s     #对后端服务器的默认检测超时时间，
							#default-server inter 1000 weight 3   #指定后端服务器的默认设置
				    -   frontend：前端，相当于nginx中的server {} ；可以有多组
				    	例如:
				    		frontend WEB_PORT(业务名称)
				    			// 格式 bind [<address>]:<port_range> [, ...] [param*]
								bind :80,:8080 #指定HAProxy的监听地址，可以是IPV4或IPV6，可以同时监听多个IP或端口，可同时用于listen字段中
								bind 192.168.7.102:10080,:8801-8810,192.168.7.101:9001-9010
								#注意：如果需要绑定在非本机的IP，需要开启内核参数：net.ipv4.ip_nonlocal_bind=1
								mode http/tcp #指定负载协议类型
								use_backend backend_name #调⽤的后端服务器组名称
				    -   backend：后端，相当于nginx中的upstream {}；可以有多组，
				    	例如:
				    		mode http|tcp     #指定负载协议类型,和对应的frontend必须一致
							option 			  #配置选项
							server   		  #定义后端real server,必须指定IP和端口
							server 配置
							针对一个server配置
							check #对指定real进行健康状态检查，如果不加此设置，默认不开启检查,check后面没有其它配置也可以启用检查功能
								  #默认对相应的后端服务器IP和端口,利用TCP连接进行周期性健康性检查,注意必须指定端口才能实现健康性检查
							 	addr <IP>    #可指定的健康状态监测IP，可以是专门的数据网段，减少业务网络的流量
							 	port <num>   #指定的健康状态监测端口
							 	inter <num>  #健康状态检查间隔时间，默认2000 ms，单位是毫秒 =2s
							 	fall <num>   #后端服务器从线上转为线下的检查的连续失效次数，默认为3
							 	rise <num>   #后端服务器从下线恢复上线的检查的连续有效次数，默认为2
							weight <weight> #默认为1，最大值为256，0(状态为蓝色)表示不参与负载均衡，但仍接受持久连接
							backup   #将后端服务器标记为备份状态,只在所有非备份主机down机时提供服务，类似Sorry Server
							disabled #将后端服务器标记为不可用状态，即维护状态，除了持久模式，将不再接受连接,状态为深黄色,优雅下线,不再接受新用户的请求
							redirect prefix http://www.baidu.com/ #将请求临时(302)重定向至其它URL，只适用于http模式
							redir http://www.baidu.com       	  #将请求临时(302)重定向至其它URL，只适用于http模式
							maxconn <maxconn> 					  #当前后端server的最大并发连接数，是HAproxy转发给后端server的最大连接数，不是HAproxy接受请求的最大连接数，很少设置
							backlog <backlog> 	#当前端服务器的连接数达到上限后的后援队列长度，注意：不支持backend
				    -   listen：同时拥有前端和后端配置，配置简单，生产推荐使用
			listen http_front // haproxy客户端页面
				bind 0.0.0.0:8100
				mod http
				stats uri /haproxy
				stats auth root:0000 // 页面登录的用户名密码
			listen rabbitmq_ha //负载均衡的名字
				bind 0.0.0.0:5600
				server rabbit1 IP:5674 check inter 2000 fall 3 ...
				server rabbit1 IP:5674 check inter 2000 fall 3 ...
				server rabbit1 IP:5674 check inter 2000 fall 3 ...
			docker(haproxy)
				8102:8100
				5602:5600
			docker(rabbitMq)
				15674:15672 // web访问端口
				5674:5672 // 程序访问端口
			访问(rabbitMq)
				IP:15674
			访问(haproxy)
				IP:8102
	keepalived:(两个Master网卡名称/priority(优先级)配置不一样)
		配置文件格式
			GLOBAL CONFIGURATION
			    Global definitions
			    例如:
			    	global_defs {  
					    router_id server1.lck.local #标识这台机器ID，默认情况下是主机名，可以配置成主机名
					    vrrp_skip_check_adv_addr 	#所有报文都检查比较消耗性能，此配置为如果收到的报文和上一个报文是同一个路由器则跳过检查报文中的源地址
					    #vrrp_iptables 				#yum安装需要加此参数，添加此参数
					    vrrp_strict 				#严格遵守VRRP协议,不允许状况:1,没有VIP地址,2.配置了单播邻居,3.在VRRP版本2中有IPv6地址
					    vrrp_garp_interval 0 		#ARP报文发送延迟
					    vrrp_gna_interval 0 		#消息发送延迟
					}
			VRRP CONFIGURATION
			    VRRP instance(s)：即一个个的vrrp虚拟路由器
			    例如:
			    	vrrp_instance VI_1 {      #虚拟路由器名称，在一个keepalived可以启多个虚拟路由器，每个虚拟路由器的名字都不一样
					    state MASTER          #当前节点在此虚拟路由器上的初始状态，状态为MASTER或者BACKUP，一般都是配置backup，该值无法决定身份，最终还是通过比较priority
					    interface eth0        #绑定为当前虚拟路由器使用的物理接口，如：ens32,eth0,bond0,br0
					    virtual_router_id 51  #每个虚拟路由器惟一标识，范围：0-255，同一组虚拟路由器的vrid必须一致
					    priority 100          #当前物理节点在此虚拟路由器的优先级，范围：1-254，每个keepalived主机节点此值不同
					    advert_int 1          #vrrp通告的时间间隔，默认1s
					    authentication {      #认证机制 
					        auth_type PASS    #AH（不推荐）或PASS
					        auth_pass 1111    #预共享密钥，仅前8位有效，同一个虚拟路由器的多个keepalived节点必须一样
					    }
					    virtual_ipaddress {                      #虚拟IP
					        10.0.0.100                           #指定VIP，不指定网卡，默认为eth0,注意：不指定/prefix,默认为/32
					        10.0.0.101/24 dev eth1               #指定VIP的网卡
					        10.0.0.102/24 dev eth2 label eth2:1  #指定VIP的网卡label
					    }
					}
			LVS CONFIGURATION
			    Virtual server group(s)
			    Virtual server(s)：ipvs集群的vs和rs
			    例如:	
			    	virtual_server 10.0.0.100 80 {    #定义虚拟主机IP地址及其端口
					    delay_loop 6    #检查后端服务器的时间间隔
					    lb_algo wrr    #定义调度方法，可选rr|wrr|lc|wlc|lblc|sh|dh
					    lb_kind DR    #集群的类型,注意要大写，可选NAT|DR|TUN
					    #persistence_timeout 20    #持久连接时长，LVS在多少时间内没有与后端服务进行数据传输，就会断开
					    protocol TCP    #指定服务协议，可选TCP|UDP|SCTP
					    sorry_server 10.0.0.101 80    #所有RS故障时，备用服务器地址（报错服务器）
					    real_server 10.0.0.11 80 {    #RS的IP和PORT
					        weight 1    #RS权重
					        notify_up <STRING>|<QUOTED-STRING>    #RS上线通知脚本 
					        notify_down <STRING>|<QUOTED-STRING>    #RS下线通知脚本
					        HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|MISC_CHECK {         #定义当前主机的健康状态 检测方法
					            ... 
					        }
					    }
					}
			# 当Keepalived配合Nginx或HAProxy的时候，我们需要检测Nginx或HAProxy的存活；我们可以利用VRRP Script进行判断，当master的Nginx或HAProxy进程不存在时，优先级降到比backup低，将VIP切换到backup；当master的Nginx或HAProxy进程恢复时，优先级升到初始值，重新抢占VIP。
				vrrp_script <SCRIPT_NAME> {    #定义一个检测脚本，在global_defs之外配置
				    script <STRING>|<QUOTED-STRING>    #shell命令或脚本路径
				    interval <INTEGER>                 #间隔时间，单位为秒，默认1秒
				    timeout <INTEGER>                  #超时时间
				    weight <INTEGER:-254..254>         #此值为负数，表示fall（（脚本返回值为非0）时，会将此值与本节点权重相加可以降低本节点权重，如果是正数，表示 rise （脚本返回值为0）成功后，会将此值与本节点权重相加可以提高本节点权重，通常使用负值较多
				    fall <INTEGER>                     #脚本几次失败转换为失败，建议设为2以上
				    rise <INTEGER>                     #脚本连续监测成功后，把服务器从失败标记为成功的次数
				    user USERNAME [GROUPNAME]          #执行监测脚本的用户或组
				    init_fail                          #设置默认标记为失败状态，监测成功之后再转换为成功状态
				}
			例如:
		1.下载安装:yum install -y keepalived
		2.配置虚拟IP(隐藏实际IP):/etc/keepalived/keepalived.conf(每个haproxy都要安装keepalived,并且配置相同的虚拟IP)
			...
			vrrp_script chk_haproxy { 							#定义一个检测脚本，在global_defs之外配置
				script "/etc/keepalived/haproxy_check.sh" // 检测haproxy状态的脚本路径
					.sh脚本文件 // 服务探测，返回0说明服务是正常的
					#!/bin/bash
					A=`ps -C haproxy --no-header |wc -l`
					if [ $A -eq 0 ];then
					        service haproxy start
					    sleep 3
					    if [ `ps -C haproxy --no-header |wc -l` -eq 0 ];then
					        service keepalived stop
						exit 1
					    fi
					fi
					exit 0
				interval 2			#间隔时间，单位为秒，默认1秒
				weight 2 #此值为负数，表示fall（（脚本返回值为非0）时，会将此值与本节点权重相加可以降低本节点权重，如果是正数，表示 rise （脚本返回值为0）成功后，会将此值与本节点权重相加可以提高本节点权重，通常使用负值较多
			}
			vrrp_instance haproxy_ha { // 指定vrrp热备参数
				state BACKUP // 服务器角色是master，备份服务器设置为BACKUP
				interface eth0 // 承载漂移ip的网卡 7的系统 ens开头 通过虚拟IP192.168.0.146或者eth0都可以访问
				virtual_router_id 51 // 定义一个热备组，可以认为这是51号热备组
				priority 100 // 主服务器优先级要比备服务器高
				advert_int 1 // 1秒互相通告一次，检查对方死了没
				authentication {
					auth_type PASS // 认证类型
					auth_pass 0000 // 认证密码
				}
				virtual_ipaddress {
					192.168.0.146 // 对外暴露的虚拟IP...
				}
			}
			...
		3.启动(进入指定配置文件形式)
			keepalievd -f /keepalievd/keepalievd.conf
		4.这样调用rabbitMq只用使用虚拟IP即可(如果是内网,可以考虑使用nginx的upstream反向代理)
nginx使用lua模块（以lua使用redis为例）
	1.nginx 通过 --add-module 加载nginx-lua模块(如果是通过yum安装， 需要通过nginx -V查看版本，然后再编译安装相同版本，编译安装的时候加载模块)
	2.nginx配置文件中，加载文件（lua_package_path "****/redis.lua;;"） #在server外层
	3.nginx配置文件中，使用content_by_lua_file *****/lua_redis; #这个文件是lua操作redis的代码，在server中,或者location中
	4.获取参数
		local request_method ngx.var.request_method;
		if "GET" == request_method then
			args = ngx.req.get_uri_args();
		elseif "POST" == request_method then
			args = ngx.req.get_post_args();
		end
		params = args["sku_id"];
主从同步过程
	<span class="image featured"><img src="{{ 'assets/images/other/mysqlbinlog.jpg' | relative_url }}" alt="" /></span>
	binlog是二进制日志文件，可使用mysql自带mysqlbinlog工具查看 mysqlbinlog myql-bin.000001
	注(binlog)：
		1.binlog文件会随服务的启动创建一个新文件
		2.通过flush logs 可以手动刷新日志，生成一个新的binlog文件
		3.通过show master status 可以查看binlog的状态
		4.通过reset master 可以清空binlog日志文件
		5.通过mysqlbinlog 工具可以查看binlog日志的内容
		6.通过执行dml，mysql会自动记录binlog
	主从搭载过程
		1.配置三台服务器并安装MySQL
		2.主库开启binlog日志，查看状态 show master status/G;
		3.在从节点配置可配置MySQL配置文件，也可执行下面的语句
			change master to
			master_host='192.168.232.101',
			master_user='repl',
			master_password='123456',
			master_log_file='mysql- bin.000001',
			master_log_pos=154;
		4.开启从节点start slave;
服务器之间加密通信
	每个节点都执行
	[root@100 ~]# cd ~
	[root@100 ~]# ssh-keygen -t rsa #看到提示不用管，一路回车就是
	[root@100 ~]# cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	[root@100 ~]# chmod 600 ~/.ssh/authorized_keys
	只要在一个节点执行即可。这里在 192.168.232.100上执行
	[root@100 ~]# ssh 192.168.232.101 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys (添加本地公钥到授权文件)
	[root@100 ~]# ssh 192.168.232.102 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
	[root@100 ~]# ssh 192.168.232.103 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
	分发整合后的文件到其它节点
	[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.101:~/.ssh/
	[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.102:~/.ssh/
	[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.103:~/.ssh/
es集群搭建
	服务器配置，三台centos虚拟机，ip列表如下：
		192.168.52.131
		192.168.52.132
		192.168.52.133
	安装es之前先安装jdk，jdk的安装略去。
	es的版本：elasticsearch-6.5.4

	三台服务器es安装路径信息

	[root@master app]# pwd
	/usr/local/app
	[root@master app]# ls
	elasticsearch-6.5.4  elasticsearch-6.5.4.zip  jdk1.8.0_191
	三台服务器配置如下：

	192.168.52.131配置信息：

	[root@master elasticsearch-6.5.4]# vim config/elasticsearch.yml
	#配置es的集群名称，默认是elasticsearch，
	#es会自动发现在同一网段下的es，
	# 如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群。
	cluster.name: cell
	#
	# ------------------------------------ Node ------------------------------------
	node.name: node_01
	node.master: true
	node.data: true
	#
	# Add custom attributes to the node:
	#
	#node.attr.rack: r1
	#
	# ----------------------------------- Paths ------------------------------------
	#
	# Path to directory where to store the data (separate multiple locations by comma):
	#
	path.data: /var/data/elasticsearch
	#
	# Path to log files:
	#
	path.logs: /var/log/elasticsearch
	network.host: 0.0.0.0
	http.port: 9200
	transport.tcp.port: 9300

	discovery.zen.ping.unicast.hosts: ["192.168.52.131:9300","192.168.52.132:9300", "192.168.52.133:9300"]

	discovery.zen.minimum_master_nodes: 2 
	192.168.52.132配置信息：

	只需要修改node.name即可：

	node.name: node_02

	192.168.52.133配置信息：

	node.name: node_03

	分别启动三台服务器上的es，（这里不能使用root用户启动，请创建个普通用户，如何创建，略去）

	查看集群信息：

	浏览器访问：http://192.168.52.131:9200/_cat/nodes?v

	显示结果如下：

	ip             heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
	192.168.52.132           13          94   6    0.11    0.26     0.23 mdi       -      node_02
	192.168.52.133           14          93   4    0.10    0.17     0.14 mdi       *      node_03
	192.168.52.131           13          92  15    0.22    0.50     0.28 mdi       -      node_01
	到此搭建成功了。。
安装go
	下载go包并安装
	配置环境变量
		vi /etc/profile, 在export后加bin路径
		重启环境变量 source /etc/profile
使用go-mysql-elasticsearch根据MySQL的binlog日志内容同步到ES
	安装插件并配置MySQL的库与表,ES的地址及索引
降级(降级主要针对不是特别重要但是比较占用资源的模块)
	1.降级的种类
		根据开关的位置分为：代码降级和前置降级
		读写降级：读降级和写降级
			读降级：数据从MySQL->缓存(redis)->静态资源->兜底文件(就是默认)->nginx返回(直接返回空)
		(nginx的限流:ngx_http_limit_req_module,ngx_http_limit_conn_module模块)
	2.手动降级,自动降级
	操作：
		1.nginx加载lua模块
		2.lua文件从redis读取开关设置来判断是从哪里获取数据(微服务/缓存/静态文件/...)
	3.漏桶原理(基本都是nginx实现),还有多(nginx+lua实现)
		需要依赖lua-resty-limit-traffic模块(是openresty的一个扩展)
		gh repo clone openresty/lua-resty-limit-traffic(下载后是多个lua文件)
		nginx加载文件
			lua_package_path "/usr/share/lua/5.1/lua-resty-redis/lib/?.lua;;/usr/share/lua/5.1/lua- resty-redis-cluster/lib/resty‘7/?.lua;;";
			lua_package_cpath "/usr/share/lua/5.1/lua-resty-redis-cluster/lib/libredis_slot.so;;";
		-- 加载nginx—lua限流模块(限流lua脚本内引入)
		local limit_req = require "resty.limit.req"
		-- 因为模块中控制粒度为毫秒级别，所以可以做到毫秒级别的平滑处理
		local lim, err = limit_req.new("my_limit_req_store", 50, 1000) #这里只是一个计算值,这里设置rate=50个请求/每秒，漏桶桶容量设置为1000个请求
		if not lim then
		  ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
		  return ngx.exit(501)
		end
		local key = ngx.var.binary_remote_addr
		local delay, err = lim:incoming(key, true)
		ngx.say(delay)
		if ( delay <0 or delay==nil ) then
		  return ngx.exit(502)
		end
		-- delay值就是当前这个请求的等待时长，这个时长是通过resty.limit.req模块计算出来的
		-- 1000以外的就溢出
		if not delay then
		  if err == "rejected" then
		    return ngx.say("1000以外的就溢出")
		    -- return ngx.exit(502)
		  end
		  ngx.log(ngx.ERR, "failed to limit req: ", err)
		  return ngx.exit(502)
		-- 加载nginx—lua限流模块
		local limit_req = require "resty.limit.req"
		-- 这里设置rate=50个请求/每秒，漏桶桶容量设置为1000个请求
		-- 因为模块中控制粒度为毫秒级别，所以可以做到毫秒级别的平滑处理
		local lim, err = limit_req.new("my_limit_req_store", 50, 1000)
		if not lim then
		  ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
		  return ngx.exit(501)
		end
		local key = ngx.var.binary_remote_addr
		local delay, err = lim:incoming(key, true)
		ngx.say(delay)
		if ( delay <0 or delay==nil ) then
		  return ngx.exit(502)
		end
		-- delay值就是当前这个请求的等待时长，这个时长是通过resty.limit.req模块计算出来的
		-- 1000以外的就溢出
		if not delay then
		  if err == "rejected" then
		    return ngx.say("1000以外的就溢出")
		    -- return ngx.exit(502)
		  end
		  ngx.log(ngx.ERR, "failed to limit req: ", err)
		  return ngx.exit(502)
		end
		-- 50-100的等待从微服务+mysql获取实时数据;（100-50）/50 =1
		if ( delay >0 and delay <=1 ) then
		  ngx.sleep(delay)
		-- 100-400的直接从redis获取实时性略差的数据;（400-50）/50 =7
		elseif ( delay >1 and delay <=7 ) then
		  local resp, err = redis_instance:get("redis_goods_list_advert")
		  ngx.say(resp)
		  return
		-- 400-1000的从静态文件获取实时性非常低的数据（1000-50）/50 =19
		elseif ( delay >7) then
		  ngx.header.content_type="application/x-javascript;charset=utf-8"
		  local file = "/etc/nginx/html/goods_list_advert.json"
		  local f = io.open(file, "rb")
		  local content = f:read("*all")
		  f:close()
		  ngx.print(content)
		  return
		end
		ngx.say("进入查询微服务+mysql") #实际中,这一步就是正常请求微服务处理数据
	4.限制每个IP的请求速度(nginx中limit_req_zone),需要下载nginx官方限流模块
	limit_req_zone $binary_remote_addr zone=one:10m rate=60r/m;
		server {
				...
					location /login.html {
					limit_req zone=one;
				...
			}
		}
CDN域名解析
	<span class="image featured"><img src="{{ 'assets/images/other/CDNCname.jpg' | relative_url }}" alt="" /></span>
	主要通过squid来实现,squid(的端口3128)的配置文件(squid.conf)修改原站的静态资源地址
		http_port 3128 accel vhost vport // 主要是这两行
		cache peer 192.168.232.204 parent 80 0 originserver #这个是cdn服务器上的squid配置源服务器地址
		(用户->源站地址->通过阿里Cname配置到CDN服务器->CDN服务的Nginx反向代理到squid->squid配置源服务器地址->加载静态文件)
		(如果源服务器关闭,squid有缓存,squid的配置文件是可以设置缓存目录的,也可以设置缓存过期时间,还是可以访问静态文件的)
		<span class="image featured"><img src="{{ 'assets/images/other/CDNconfig.jpg' | relative_url }}" alt="" /></span>
布隆过滤器(bloomfilter)
	(redis加载扩展模块,是通过下载扩展,然后在配置文件中loadmodule加载)
	首先将位数组进行初始化,将里面每个位都设置位0.对于集合里面的每一个元素,将元素依次通过3个哈希函数进行映射,每次映射都会产生一个哈希值,这个值对应位数组上面的一个点,然后将位数组对应的位置标记为1.查询W元素是否存在集合中的时候,同样的方法将W通过哈希映射到位数组上的3个点.如果3个点的其中有一个点不为1,则可以判断该元素一定不存在集合中.反之,如果3个点都为1,则该元素可能存在集合中.注意:此处不能判断该元素是否一定存在集合中,可能存在一定的误判率.可以从图中可以看到:假设某个元素通过映射对应下标为4,5,6这3个点.虽然这3个点都为1,但是很明显这3个点是不同元素经过哈希得到的位置,因此这种情况说明元素虽然不在集合中,也可能对应的都是1,这是误判率存在的原因.

	loadmodule /usr/local/redis/redisbloom-1.1.1/rebloom.so #INITIAL_SIZE 800000 ERROR_RATE 0.1
	#位向量长度100M，误差率千分之一（百分之0.1）
	[root@redis]# redis-server redis.conf
	主要命令有
	bf.add 添加元素
	bf.exists 查询元素是否存在
	bf.madd 一次添加多个元素
	在 redis 中有两个值决定布隆过滤器的准确率：
	error_rate：允许布隆过滤器的错误率，这个值越低过滤器的位数组的大小越大，占用空间也就越大。
	initial_size：布隆过滤器可以储存的元素个数，当实际存储的元素个数超过这个值之后，过滤器的准确率会下降。
	redis 中有一个命令可以来设置这两个值：
	示例：
	php-redis扩展中有个函数可以调用原始的redis指令：
	添加：在向redis set值的之后，调用bf.add添加到过滤器。 检查：在穿过了redis去到Mysql之前，调用bf.exists检查
	一下，如果不存在。
	布隆过滤器可以用在查询和写入分开的业务模式下，一个业务会把key写入redis和BF，另一个业务来搜索查找这个
	key。 如果get和set在同一个地方就不能用BF啦。
	6 解决本节刚开始提出的缓存穿透问题
	这里我们按着本节5.2的方式来实现，实现起来相对简单些
	bf.reserve test 0.1 100000000
	第一个值是过滤器的名字。
	第二个值为 error_rate 的值。
	第三个值为 initial_size 的值。
	注意必须在add之前使用bf.reserve指令显式创建，如果对应的 key 已经存在，bf.reserve会报错。同时设置的错误率
	越低，需要的空间越大。如果不使用 bf.reserve，默认的error_rate是 0.01，默认的initial_size是 100。
	#新建一个过滤器
	bf.reserve test 0.1 100000000  # test是布隆过滤器名称，0.1是误判率，100000000是位向量长度
	#向过滤器中添加元素
	127.0.0.1:6379> bf.add test abc123 #test是布隆过滤器名称，abc123是需要判断的元素
	#判断元素是否在过滤器中
	127.0.0.1:6379> bf.exists test abc123 #test是布隆过滤器名称，abc123是需要判断的元素
	1
	// php实现布隆过滤器
	$redis = new \Redis();
	$redis->connect('127.0.0.1', 6379);
	//先看看布隆过滤器中验证数据是否存在， 如果布隆过滤器说没有，那就肯定没有
	$re = $redis->rawCommand('bf.exists', 'goods_sku_code', '商品编码123456abcdefg');
	if($re == 0){
	  die("商品编码不存在");
	}else{
	  //这里写查询mysql代码
	}
	var_dump($re);
	<span class="image featured"><img src="{{ 'assets/images/other/redisBloomFilter.jpg' | relative_url }}" alt="" /></span>
	注:(如果使用PHP+redis实现布隆过滤器的话,主要使用setbit与getbit函数)
	(为什么使用两个hash函数,这样计算结果不同,减少误判率)
	<span class="image featured"><img src="{{ 'assets/images/other/RedisBloomFilterliucheng.jpg' | relative_url }}" alt="" /></span>
	使用场景:消耗内存过多的数据
		比如敏感词,身份证号等等
		1.Google著名的分布式数据库Bigtable以及Hbase使用了布隆过滤器来查找不存在的行或列以减少磁盘查找的ＩＯ
		2.检查垃圾邮件地址
		3.Google chrome 浏览器使用bloom filter识别恶意链接
		4.文档存储检索系统也采用布隆过滤器来检测先前存储的数据
		5.爬虫URL地址去重 A,B 两个文件
		6.解决缓存穿透问题:缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞)
分库分表
	1.表数据过大导致查询问题
	2.降低压力,降低负载
	3.容灾
	映射关系,根据某个字段进行取模,根据映射关系确认哪个库
	逻辑:
		订单表进行hash分库分表
		再根据用户和商家进行分库分表(会出现数据冗余),这样针对店铺/用户查询就可在指定库指定表查询信息
	数据统计问题:
		1.使用ES进行统计
		2.使用统计库(包含各种统计数据,相当于给统计单独拉出来一个库,使用Rabbitmq进行累加)
	热点数据单独处理(比如较大数据量的商家单独使用数据库,相当于大客户优先)
	数据迁移:
		1.脚本根据新的算法查询重新插入新库
		2.数据都复制过去,把不需要的数据剔除
	新数据插入问题：
		1.停服维护
		2.用户活跃度低的时间段
分布式事务(主要解决方案)
	2pc(两阶段提交,MySQL官方是指XA事务)
		$dbtest1 = new db("rdsg4hgebq8827g143m5o.mysql.rds.aliyuncs.com","query1","cdS1234567","base0");
		$dbtest2 = new db("39.99.165.81","root","cdS1234567","atest");
		//为XA事务指定一个id，xid 必须是一个唯一值。
		$xid = uniqid("");
		//两个库指定同一个事务id，表明这两个库的操作处于同一事务中
		$dbtest1->exec("XA START '$xid'");//准备事务1
		$dbtest2->exec("XA START '$xid'");//准备事务2
		try {
		    //$dbtest1
		    $return = $dbtest1->exec("UPDATE atest SET id=3 WHERE id=2") ;
		    echo "xa1:"; print_r($return);
		    if(!in_array($return,['0','1'])) {
		        throw new Exception("库1执行sql操作失败！");
		    }

		    //$dbtest2
		    $return = $dbtest2->exec("UPDATE atest_2 SET id2=3 WHERE id2=2") ;
		    echo "xa2:"; print_r($return);
		    if(!in_array($return,['0','1'])) {
		        throw new Exception("库2执行sql操作失败！");
		    }

		    //阶段1：$dbtest1提交准备就绪
		    $dbtest1->exec("XA END '$xid'");
		    $dbtest1->exec("XA PREPARE '$xid'");

		    //阶段1：$dbtest2提交准备就绪
		    $dbtest2->exec("XA END '$xid'");
		    $dbtest2->exec("XA PREPARE '$xid'");

		    //阶段2：提交两个库
		    $dbtest1->exec("XA COMMIT '$xid'");
		    $dbtest2->exec("XA COMMIT '$xid'");
		} catch (Exception $e) {
		    //阶段2：回滚
		    $dbtest1->exec("XA ROLLBACK '$xid'");
		    /*上面这行代码是2pc中的xa事务，
		    update set a = a+1
		    如果是TCC，那么上面这行代码就变了，变成调用一个php接口，这个接口的作用就是把之前的操作给取消
		    update set a = a-1*/

		    $dbtest2->exec("XA ROLLBACK '$xid'");
		    die("Exception:".$e->getMessage());

		}
		<span class="image featured"><img src="{{ 'assets/images/other/MysqlXa.jpg' | relative_url }}" alt="" /></span>
		存在的问题：
			1.性能问题。从流程上我们可以看得出，其最大缺点就在于它的执行过程中间，节点都处于阻塞状态。各个操作数据库的节点此时都占用着数据库资源，只有当所有节点准备完毕，事务协调者才会通知进行全局提交，参与者进行本地事务提交后才会释放资源。这样的过程会比较漫长，对性能影响比较大。
			2.协调者单点故障问题。事务协调者是整个XA模型的核心，一旦事务协调者节点挂掉，会导致参与者收不到提交或回滚的通知，从而导致参与者节点始终处于事务无法完成的中间状态。
			3.丢失消息导致的数据不一致问题。在第二个阶段，如果发生局部网络问题，一部分事务参与者收到了提交消息，另一部分事务参与者没收到提交消息，那么就会导致节点间数据的不一致问题。
	3pc(三阶段提交)
		三阶段提交又称3PC，其在两阶段提交的基础上增加了CanCommit阶段，并引入了超时机制。一旦事务参与者迟迟没有收到协调者的Commit请求，就会自动进行本地commit，这样相对有效地解决了协调者单点故障的问题。

		但是性能问题和不一致问题仍然没有根本解决,所以没有代码实例。

		在阶段一中，如果所有的参与者都返回Yes的话，那么就会进入PreCommit阶段进行事务预提交。此时分布式事务协调者会向所有的参与者节点发送PreCommit请求，参与者收到后开始执行事务操作，并将Undo和Redo信息记录到事务日志中。参与者执行完事务操作后（此时属于未提交事务的状态），就会向协调者反馈“Ack”表示我已经准备好提交了，并等待协调者的下一步指令。

		否则，如果阶段一中有任何一个参与者节点返回的结果是No响应，或者协调者在等待参与者节点反馈的过程中超时（2PC中只有协调者可以超时，参与者没有超时机制）。整个分布式事务就会中断，协调者就会向所有的参与者发送“abort”请求。

		相比较2PC而言，3PC对于协调者（Coordinator）和参与者（Partcipant）都设置了超时时间，而2PC只有协调者才拥有超时机制。这解决了一个什么问题呢？这个优化点，主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。而这种机制也侧面降低了整个事务的阻塞时间和范围。

		另外，通过CanCommit、PreCommit、DoCommit三个阶段的设计，相较于2PC而言，多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的。

		以上就是3PC相对于2PC的一个提高（相对缓解了2PC中的前两个问题），但是3PC依然没有完全解决数据不一致的问题。

		3pc解决了2阶段提交的前面2个问题， 但是后面那个问题依旧没有解决
	TCC(补偿事务)
		其核心思想是："针对每个操作都要注册一个与其对应的确认和补偿（撤销操作）"。
		它分为三个操作：
			Try阶段：主要是对业务系统做检测及资源预留。(类似预锁库存逻辑)
			Confirm阶段：确认执行业务操作。 通过调用确认接口(类似减库存逻辑)
			Cancel阶段：取消执行业务操作。 通过调用取消接口(类似加库存逻辑)
		TCC事务的处理流程与2PC两阶段提交类似(也可以说TCC也是一种2PC提交)，不过2PC通常都是在跨库的DB层面，而TCC本质上就是一个应用层面的2PC，需要通过业务逻辑来实现。这种分布式事务的实现方式的优势在于，可以让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。

		而不足之处则在于对应用的侵入性非常强，业务逻辑的每个分支都需要实现try、confirm、cancel三个操作。此外，其实现难度也比较大，需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口还必须实现幂等。
	MQ(最大努力通知型)
		主要思路就是分为生产者与消费者(消费者采用nack与ack方法),这样并不是用到事务处理(生产者与消费者内部是通过事务处理的),而是通过队列来保证数据前后的一致性
		1.生产者确保消息投递成功,采用记录投递状态(成功/失败),如果失败,会有定时脚本查询并重新投递
		2.消费者同样确保消息处理成功.
		方案1：
			<span class="image featured"><img src="{{ 'assets/images/other/mysqlMqTransfer.jpg' | relative_url }}" alt="" /></span>
		方案2：
			<span class="image featured"><img src="{{ 'assets/images/other/mysqlMqtransfer2.jpg' | relative_url }}" alt="" /></span>
数据统计(订单等)
	1.基础方法,使用sql的count(*)统计(不推荐)
	2.建立一个统计数据的库来维护(分布式事务解决数据一致性问题,可采用第8条的MQ方式)

	例:(生产者)
	/*插入数据到订单表*/
	/*实际场景中，这里要加事务*/
    $diff_struts_db = new \App\Components\Order\DiffStrutsDb();
    $insert_result = $diff_struts_db->insert(['data'=>['id'=>time()]]);

    /*
     * 消息推送到rabbitmq
     */
    $exchange = 'exchange_1';
    $queue = 'order_satistic_queue';

    //获得rabbitmq集群配置
    $config = bean('config')->get('rabbitmq.rabbitmq_1');

    //连接broker,创建一个rabbitmq连接
    $connection = new AMQPStreamConnection($config['host'], $config['port'], $config['login'], $config['password'], $config['vhost']);

    //创建一个通道
    $channel = $connection->channel();

    /*这个代码是rabbitmq高级特性：comfirm机制*/
    /*监听器*/
    /*其实不加监听器也是可以的,就只是把队列推送过去,如果加了监听器的话,可以获取到队列推送的成功与否*/
    //监听到推送成功就：
    $channel->set_ack_handler(
        function (AMQPMessage $message) {
            //update 订单表 set is_send_succ=ture
            echo "Message acked with content " . $message->body . PHP_EOL;

            /*这里省略以下逻辑（失败重试逻辑）：*/
            //把推送成功的记录起来。
            //插入到订单推送状态表，推送状态字段默认是0，推送成功才会把这个字段修改为1，推送失败这个字段的值就是0
            /*还要写一个脚本，去重试失败记录，也就是重试哪些推送状态字段的值是0的记录*/
        }
    );

    //监听到推送失败就：
    $channel->set_nack_handler(
        function (AMQPMessage $message) {
            //update 订单表 set is_send_succ=false
            echo "Message nacked with content " . $message->body . PHP_EOL;

            /*这里省略以下逻辑（失败重试逻辑）：*/
            //把推送失败的记录起来。
            //插入到订单推送状态表，推送状态字段默认是0，推送成功才会把这个字段修改为1，推送失败这个字段的值就是0
            /*还要写一个脚本，去重试失败记录，也就是重试哪些推送状态字段的值是0的记录*/
        }
    );

    //申明comfirm机制
    $channel->confirm_select();

    //申明队列
    $channel->queue_declare($queue, false, true, false, false);

    //申明交换机
    $channel->exchange_declare($exchange, AMQPExchangeType::FANOUT, false, false, true);
    //将交换机和队列绑定
    $channel->queue_bind($queue, $exchange);

    /*写死一条测试消息*/
    $messageBody = json_encode( [['sku_id'=>123,'num'=>2,'type'=>'crateOrder','order_no'=>'T3433335']]);

    /*把消息转化成rabbitmq消息格式*/
    $message = new AMQPMessage($messageBody, array('content_type' => 'text/plain', 'delivery_mode' => AMQPMessage::DELIVERY_MODE_PERSISTENT));

    /*推送这条消息*/
    $channel->basic_publish($message, $exchange);

    $channel->wait_for_pending_acks(3);
    $channel->close();
    $connection->close();

    return [[$insert_result]];
    (为了防止并发,可将唯一索引+事务来处理)

	例:(消费者)
	require('../config/config.php');
    //连接broker,创建一个rabbitmq连接
    $connection = new AMQPStreamConnection($config['host'], $config['port'], $config['login'], $config['password'], $config['vhost']);
    if(!$connection){
        echo "连接失败";
        exit();
    }
    /*声明一个通道*/
    $channel = $connection->channel();
    /*声明一个队列*/
    $channel->queue_declare('order_satistic_queue', false, true, false, false);
    /*每次消费限流1000,防止队列挂掉*/
    $channel->basic_qos(null, 1000, null);
    /*开启消费监听*/
    $channel->basic_consume('order_satistic_queue', '', false, false, false, false, function ($message)
    {
        echo "\n--------\n";
        echo $message->body;
        echo "\n--------\n";
        /*从rabbitmq里面取出json数据*/
        $data_json = $message->body;
        /*把json数据编译后，放入$data变量，
        $data变量是个二维数组，代表多条mysql数据*/
        $data = json_decode($data_json, true);
        /*数据保存到mysql数据库*/
        saveToMysql($data, $message);
    });
    while ($channel->is_consuming()) {
        // 测试时10秒后让进程死掉，正式环境这里第三个参数要填0而不是10
        $channel->wait(null, false, 0);
    }
    // 消费执行代码
    try {
        $db = new db("39.99.165.81","root","cdS1234567","atest");
        $is_has = $db->getOne(['table'=>'order_log','where_str'=>'AND order_no=450']) ;
        if($is_has){
            /*这里应该还有其它逻辑的*/
            $message->ack(true);// 这一行代码，底层是给rabbitmq推送一条消息，告诉他我消费成功，你可以删除消息了
            return;
        }
        /*开启事务*/
        $db->begin();
        $return1 = $db->exec("修改统计库的sql") ;
        $return2 = $db->exec("INSERT INTO order_log values (订单号)") ;
        echo "执行结果:"; print_r($return1); print_r($return2);
        /*
         *提交事务 注:这个就是幂等，其实就是通过这种形式(此处是唯一索引)把统计加在事务中，这样实现
         */
        if(!$db->commit()) {
            throw new Exception("执行sql操作失败！");
        }
        /*成功ack*/
        $message->ack(true);// 这一行代码，底层是给rabbitmq推送一条消息，告诉他我消费成功，你可以删除消息了
    } catch (Exception $e) {
        //print $e->getMessage();
        $db->rollBack();
        /*注意，这里nack要加一个次数限制，代码略，防止无限循环
        如果多次都失败，可以把它存到日志里面，再手动处理
        */
        /*失败nack*/
        $message->nack(true);// 这一行代码，底层是给rabbitmq推送一条消息，告诉他我消费失败，你不要删除消息
    }
消息队列(RabbitMQ)
	生产端:
		主要使用confirm机制(set_ack_handler/set_nack_handler监听器),来确保消息推送成功/失败
	消费端
		主要使用ACK机制($message->ack()/$message->nack()),来确认消费成功/失败
Kafka
	特性：
		以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间的访问性能。
		高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条消息的传输。(在消息确认模块,rabbitmq效果更好)
		支持Kafka Server间的消息分区，及分布式消费，同时保证每个partition内的消息顺序传输。
		同时支持离线数据处理和实时数据处理。
		Scale out:支持在线水平扩展
	场景：
		1.消息系统
		2.应用监控：CPU占用率，IO，内存使用，QPS等
		3.网站用户追踪：用户操作习惯等
		4.流处理
		5.存储日志
	PHP使用：
		安装composer require psr/log=1.0.2 与 composer require nmred/kafka-php 扩展
	注：
		kafka是通过偏移量(查找方式如下图)来记录,消费依次向下,不像Mq消费完会删除掉
			<span class="image featured"><img src="{{ 'assets/images/other/kafkaindex.jpg' | relative_url }}" alt="" /></span>
	生产者代码：
		<?php require './vendor/autoload.php';
		date_default_timezone_set('PRC');
		$config = \Kafka\ProducerConfig::getInstance();
		$config->setMetadataRefreshIntervalMs(10000);
		$config->setMetadataBrokerList('192.168.232.204:9093'); // 多个用逗号隔开
		$config->setBrokerVersion('1.0.0');
		$config->setRequiredAck(1);
		$config->setIsAsyn(false); // false同步/true异步
		$config->setProduceInterval(500);
		$producer = new \Kafka\Producer();
		for($i = 0; $i < 2; $i++) {
			$result = $producer->send([
				[
					'topic' => 'test', 'value' => 'test1....message.', 'key' => ''
				]
			]);
			var_dump($result);
		}
	消费者代码：
		require './vendor/autoload.php';
		date_default_timezone_set('PRC');
		$config = \Kafka\ConsumerConfig::getInstance();
		$config->setMetadataRefreshIntervalMs(10000);
		$config->setMetadataBrokerList('192.168.232.204:9093'); // 多个用逗号隔开
		$config->setGroupId('test'); // 同一分组下的消费者,多个中只有一个可以进行消费,不会重复消费, 如果是不同分组下,则会同时收到信息
		$config->setBrokerVersion('1.0.0');
		$config->setTopics(array('test'));
		//$config->setOffsetReset('earliest');
		$consumer = new \Kafka\Consumer();
		#开启消费
		$consumer->start(function ($topic, $part, $message) {
			var_dump($message);
			//打印出获取的消息
		});
秒杀(超卖问题)
	nginx漏桶原理,更少的请求进入程序
	1.MySQL悲观锁(不推荐,因为一直都在排队等待锁的释放)
		指在数据库层面进行加锁,一般是用for update
	2.MySQL乐观锁(如果有多个库存,会导致下单成功的不一定是靠前的)
		指在业务层进行加锁,例如
			$version = mysql查询"select version from table"
			...其他正常业务
			mysql执行"update table set version = $version+1 where version = $version";
		如果在处理正常业务时,有其他进程执行了这个,那么最后的update就会失败
	3.PHP+队列(单通道,必须一条一条处理,需要PHP循环处理)
		例如PHP+Redis(lpush,rpop)
	4.PHP+Redis分布式锁及锁的优化(相比于5,这个锁的是线程,如果是while循环其他的需要等待,或者与2有相同的问题)
		Redis的setnx
		针对多个,可尝试增加锁的数量
	5.PHP+Redis乐观锁(redis的watch)
		<?php
			header("content-type:text/html;charset=utf-8");
			$redis = new redis();
			$result = $redis->connect('127.0.0.1', 6379);
			$mywatchkey = $redis->get("");
			$rob_total = 10; //抢购数量
			if($mywatchkey<$rob_total){
				$redis->watch("mywatchkey");
				$redis->multi();
				//设置延迟，方便测试效果。
				sleep(5);
				//插入抢购数据
				$redis->hSet("mywatchlist","user_id_".mt_rand(1, 9999),time());
				$redis->set("mywatchkey",$mywatchkey+1);
				$rob_result = $redis->exec();
				if($rob_result){
					$mywatchlist = $redis->hGetAll("mywatchlist");
					echo "抢购成功！";
					echo "剩余数量：".($rob_total-$mywatchkey-1)."";
					echo "用户列表：";
					var_dump($mywatchlist);
				}else{
					echo "手气不好，再抢购！";
					exit;
				}
			}
		?>
		// 主要是这四个步骤
		$redis->watch("mywatchkey"); //声明一个乐观锁
		$redis->multi(); //redis事务开始
		$redis->set("mywatchkey",$mywatchkey+1); //乐观锁的版本号+1
		$rob_result = $redis->exec();//redis事务提交
	注：更快的是nginx+lua实现(redis的watch)
		注意：lua中,ng.say("")与ng.exec("")不能同时使用
事务与锁
	update操作时
		如果使用一般索引
			会出现间隙锁(页锁)
		无索引
			表锁
		唯一索引
			行锁
	减少死锁
		操作完成今早结束事务
		一次性分配使用到的锁
		采用乐观锁
		设置超时
		尽量减少长事务
		多个表涉及多个行的时候,加锁顺序尽量保持一直
		创建合理的索引
	1.读未提交(读取到另一个事务未提交的内容-脏读)
	2.不可重复读(一个事务内读取到另一个事务提交的数据,导致在同一个事务中前后读取数据不一致-不可重复读)
	3.可重复读(默认解决了前两个的问题,但是会出现幻读,当然,如果加上了排他锁,也可以解决幻读的问题)
		幻读，并不是说两次读取获取的结果集不同，幻读侧重的方面是某一次的 select 操作得到的结果所表征的数据状态无法支撑后续的业务操作。更为具体一些：select 某记录是否存在，不存在，准备插入此记录，但执行 insert 时发现此记录已存在，无法插入，此时就发生了幻读。
	4.序列化(解决了幻读)
	Innodb的行级锁是在索引的基础上的，如果没有索引，就会锁表
	没有索引：表锁
	普通索引：间隙锁(取决于操作行所在前后的值比如[10,20),防止幻读)
	主键/唯一索引：行锁
日志系统(主要采用ELK es+logstash+kibana)
	<span class="image featured"><img src="{{ 'assets/images/other/elkmain.jpg' | relative_url }}" alt="" /></span>
	Filebeat
		安装(解压即可使用)
			需要修改配置文件filebeat.yml
				#======================= Filebeat inputs =============================
				filebeat.inputs:
				- type: log #输入类型
				  enabled: true #启用或禁用这段配置
				  paths: #日志文件名正则匹配
				    - ${ACSDIR}/logs/*.log
				    - ${ACSDIR}/logs/*.log.20[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]
				  encoding: gbk #日志编码
				  include_lines: ['WARN -', 'ERROR-', 'INFO -', 'DEBUG-'] #采集包含指定内容的行
				  exclude_files: ['\.swp$'] #需要排除日志文件的正则表达
				  tags: ["init-serice"] #标识

				- type: log #同上
				  enabled: true
				  paths:
				    - ${ACSDIR}/logs/record/*.[0-9]*.log
				    - ${ACSDIR}/logs/record/*.[0-9]*.log.20[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]
				  encoding: gbk
				  exclude_files: ['\.swp$']
				  tags: ["init_service_record"]

				#======================= Filebeat modules===========================
				filebeat.config.modules: #filebeat内部模块的配置
				  path: ${path.config}/modules.d/*.yml
				  reload.enabled: false

				#========================= Outputs ================================
				#----------------------------- Logstash output --------------------------------
				output.logstash: #指定Logstash作为输出
				  enabled: false #启用或禁用
				  hosts: ["10.17.0.8:5044","10.17.0.6:5044"] #ip、port
				  #loadbalance: true #负载均衡

				#------------------------------- Redis output ---------------------------------
				output.redis: #指定redis作为输出
				  enabled: true #启用或禁用
				  hosts: ["10.17.0.6:7001"] #ip、port

				#=========================Xpack Monitoring =========================
				xpack.monitoring: #监控配置
				  enabled: true #启用或禁用
				  elasticsearch:
				   hosts: ["10.17.0.6:9200"] #ip、port
				   username: beats_system
				   password: beatspassword
		启动
			./filebeat -e -c filebeat.yml -d "publish"
		1.logstash占用系统资源多一些,Filebeat占用极少的系统资源
		2.启动时, 监控多个日志文件,每个日志文件都有一个harvester(负责读取单个文件的内容,读取的最后一个偏移量,并确保发送所有日志行,保证事件将至少一次传递到配置的输出,并且不会丢失数据.Filebeat之所以能够实现此行为，是因为它在注册表文件中存储了每个事件的传递状态)
	Logstash
		安装(解压文件即可使用bin/logstash)
			在logstash安装目录下新建一个文件first-pipeline.conf // 自定义名称
			input {
				beats {
					port => "5044"
				}
			}
			output {
				elasticsearch {
					# hosts => ["192.168.232.104:9200","192.168.232.104:9201","192.168.232.104:9202"]
					hosts => ["192.168.232.104:9200"] // 修改为自己的es地址和端口
					index => "nginx-access-log-%{+YYYY.MM.dd}" // es的索引名称
				}
			}
		启动
			bin/logstash -f first-pipeline.conf(自定义的配置文件名称) --config.test_and_exit / --config.reload.automatic
			(--config.test_and_exit选项的意思是解析配置文件并报告任何错误)
			(--config.reload.automatic选项的意思是启用自动配置加载，以至于每次你修改完配置文件以后无需停止然后重启Logstash)
gitlab
	服务器安装Gitlab,自主搭建,直接访问图形化界面,就可以注册(相当于本地的github)
jenkins
	安装
		修改配置文件/etc/sysconfig/jenkins(比如修改端口号,用户权限...)
		注:如果有必要,更换下载插件源
			cd /var/lib/jenkins/updates/
			sed -i 's/www.google.com/www.baidu.com/g' default.json
			sed -i 's/updates.jenkins-ci.org\/download/mirrors.tuna.tsinghua.edu.cn\/jenkins/g' default.json
			sed -i 's/updates.jenkins.io\/download/mirrors.tuna.tsinghua.edu.cn\/jenkins/g' default.json
			systemctl start jenkins
	启动
		service jenkins start/restart
	查看管理员密码
		cat /var/lib/jenkins/secrets/initialAdminPassword
	网页登录
		设置用户,gitlab地址,定时构建/轮询SCM构建等
	git回滚
		方法一:(回滚)
			git log(查看想要回滚的版本号)
			git reset
				--soft(本地版本不会回滚到指定版本,仅仅移动当前Head指针,不会改变工作区和暂存区的内容,比如本地逻辑修改后可上线,本地代码就不需要回滚)
				--mixed(是reset的默认参数,移动head指针,改变暂存区内容,但不会改变工作区)
				--hard(本地代码会被回滚到指定版本,工作区和暂存区内容全部改变,比如把打印输入提交到了生产环境,本地也需要回滚)
				版本号ID
			git push origin 远程分支名称(降回滚的版本推送到远程,如果报错,是因为版本比远程的要旧,所以使用 -f 强制推送即可)
		方法二:(反做)
			git log(查看想要回滚的版本号)
			git revert 版本号ID
				撤销某次操作,此次操作之前和之后的commit和history都会保留,并且把这次撤销作为一次最新的提交,提交一个新的版本,将需要revert的版本的内容再反向修改回去,版本会递增,不影响之前提交的内容(此时有可能有冲突,解决冲突)
			git commit -m 版本名
			git push origin 远程分支名称
</pre>