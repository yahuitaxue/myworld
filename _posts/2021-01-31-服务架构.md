---
title: 服务架构
author: Yahui
layout: php
category: PHP
---


书名：《-》

<pre style="text-align: left;">
	1.nginx使用upstream模块配置负载均衡(名字叫swoft_nginx)(在location中配置反向代理到负载均衡的 swoft_nginx)
	2.配置服务发现
		nginx配置文件upstream模块使用upsync加载consul
		upstream swoole_consul {
			server 192.168.169.140:9001; #留一个固定服务否则nginx启动报错
			upsync 127.0.0.1:8500/v1/kv/upstreams/swoole_test upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=on;
			upsync_dump_path /redis_2004/17/conf/servers_test.conf; #从consul读取的信息生成配置文件
			include /redis_2004/17/conf/servers_test.conf; #引入这个配置文件
		}
		解释：
			127.0.0.1:8500/v1/kv/upstreams -> 连接consul的api资源地址
			swoole_test -> 相当于我们自己在consul中自定义的key
			upsync_timeout -> 超时时间6分钟
			upsync_interval -> 定时获取信息的时间
			upsync_type -> 类型
			strong_dependency=on; -> 是否依赖consul运行
			upsync_dump_path -> 拉取之后申请配置文件
	3.在nginx中配置好consul后,向consul增加swoft的集群地址,这样nginx就可以动态加载swoft
	总结:
		nginx使用upsync动态获取consul中的服务信息, strong_dependency是否依赖, 获取服务信息后, 生成一份配置文件(比如swoft_server.conf)
		1.构建3swoft,1nginx,1consul
		2.配置nginx upstream 异步访问consul服务端
		3.consul:实际在程序自动注册信息到consul (当时使用的是curl)
	3.docker-compose配置(部分代码)
		# 编排php,redis,nginx容器
		version: "3.6" # 确定docker-composer文件的版本
		services: # 代表就是一组服务 - 简单来说一组容器
		  # server
		  consul_master_server_170_30: # 这个表示服务的名称，课自定义; 注意不是容器名称
		    image: consul1.4 # 指定容器的镜像文件
		    ports: # 配置容器与宿主机的端口
		      - "8500:8500"
		    networks: # 引入外部预先定义的网段
		       consul:
		         ipv4_address: 170.200.7.30   #设置ip地址
		    container_name: consul_master_server_170_30 # 这是容器的名称
		    volumes: #配置数据挂载
		      - 挂载目录(测试可以指定同一目录(因为在同一服务器上, 集群环境可以使用git+Jenkins), 实现代码同步)
		    command: ./consul agent -server -bootstrap-expect 3 -data-dir /tmp/consul -node=consul_master_server_170_30 -bind=170.200.7.30 -ui -client=0.0.0.0
		    depends_on: # 容器启动依赖的顺序
		    	- mongod_shard_A_174_2
		    注：
		    	-server // 代表是一个服务
		    	-bootstrap-expect 3 // 只在master节点，表示启动多少个节点（可以没有）
		    	-data-dir /tmp/consul
		    	-node=consul_master_server_170_30 // 节点名称
		    	-bind=170.200.7.30 // 绑定网络的通信方式，有IP或者端口映射
		    	-ui // consul的web界面
		    	-client=0.0.0.0
		    	-join=170.200.7.30  // 绑定到哪个节点，从节点（slave）会有
	4.swoft
		可以利用注册与停止时间增加服务发现事件(官方提供的有方法(注:参数是服务的名称)),来实swoft的注册与删除
	5.docker-compose配置多个服务的问题
		问题:代码同步指定同一目录(swoft), 而swoft的配置文件是同一个,导致服务发现只能发现一个
		处理:使用服务器启动时的ip地址来配置.env文件,swoft启动读取.env文件来配置,从而实现同一套代码配置文件是多套
	6.在swoft的listener下的RegisterServiceListener中,配置check模式
		"check" => [
			"name" => "swoft.goods.server" // 这个主要是区分与其他
			"tcp" => "你的swoft宿主机地址" // 这个如果是http，可以写成http的地址(http://test.com)
			"interval" => "10s" // 如果及时性比较低,可以设置长一些,这样对服务器压力小一点
			"timeout" => "2s" // 设置超时时间
		] 这个是用到swoft-consul组件, 这样consul就会生成一个定时器,检测swoft的健康状况
		总之,利用docker-composer,增加启动命令,给shell脚本传递IP,端口,shell脚本处理传递过来的参数,进行拼接,最终写进.env文件中,(形如HOST=12.12.12.11....),swoft启动读取.env文件内容获取IP及端口(例如第6点,将swoft地址拼接到swoft-consul组件配置项中),最终达到consul定时检测通过docker-composer启动的swoft服务
	7.nginx使用lua模块（以lua使用redis为例）
		1.nginx 通过 --add-module 加载nginx-lua模块(如果是通过yum安装， 需要通过nginx -V查看版本，然后再编译安装相同版本，编译安装的时候加载模块)
		2.nginx配置文件中，加载文件（lua_package_path "****/redis.lua;;"） #在server外层
		3.nginx配置文件中，使用content_by_lua_file *****/lua_redis; #这个文件是lua操作redis的代码，在server中,或者location中
		4.获取参数
			local request_method ngx.var.request_method;
			if "GET" == request_method then
				args = ngx.req.get_uri_args();
			elseif "POST" == request_method then
				args = ngx.req.get_post_args();
			end
			params = args["sku_id"];
	8.主从同步过程
		<span class="image featured"><img src="{{ 'assets/images/other/mysqlbinlog.jpg' | relative_url }}" alt="" /></span>
		binlog是二进制日志文件，可使用mysql自带mysqlbinlog工具查看 mysqlbinlog myql-bin.000001
		注(binlog)：
			1.binlog文件会随服务的启动创建一个新文件
			2.通过flush logs 可以手动刷新日志，生成一个新的binlog文件
			3.通过show master status 可以查看binlog的状态
			4.通过reset master 可以清空binlog日志文件
			5.通过mysqlbinlog 工具可以查看binlog日志的内容
			6.通过执行dml，mysql会自动记录binlog
		主从搭载过程
			1.配置三台服务器并安装MySQL
			2.主库开启binlog日志，查看状态 show master status/G;
			3.在从节点配置可配置MySQL配置文件，也可执行下面的语句
				change master to
				master_host='192.168.232.101',
				master_user='repl',
				master_password='123456',
				master_log_file='mysql- bin.000001',
				master_log_pos=154;
			4.开启从节点start slave;
	9.服务器之间加密通信
		每个节点都执行
		[root@100 ~]# cd ~
		[root@100 ~]# ssh-keygen -t rsa #看到提示不用管，一路回车就是
		[root@100 ~]# cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
		[root@100 ~]# chmod 600 ~/.ssh/authorized_keys
		只要在一个节点执行即可。这里在 192.168.232.100上执行
		[root@100 ~]# ssh 192.168.232.101 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
		[root@100 ~]# ssh 192.168.232.102 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
		[root@100 ~]# ssh 192.168.232.103 cat ~/.ssh/id_rsa.pub >>~/.ssh/authorized_keys
		分发整合后的文件到其它节点
		[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.101:~/.ssh/
		[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.102:~/.ssh/
		[root@100 ~]# scp ~/.ssh/authorized_keys 192.168.232.103:~/.ssh/
	10.es集群搭建
		服务器配置，三台centos虚拟机，ip列表如下：
			192.168.52.131
			192.168.52.132
			192.168.52.133
		安装es之前先安装jdk，jdk的安装略去。
		es的版本：elasticsearch-6.5.4

		三台服务器es安装路径信息

		[root@master app]# pwd
		/usr/local/app
		[root@master app]# ls
		elasticsearch-6.5.4  elasticsearch-6.5.4.zip  jdk1.8.0_191
		三台服务器配置如下：

		192.168.52.131配置信息：

		[root@master elasticsearch-6.5.4]# vim config/elasticsearch.yml
		#配置es的集群名称，默认是elasticsearch，
		#es会自动发现在同一网段下的es，
		# 如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群。
		cluster.name: cell
		#
		# ------------------------------------ Node ------------------------------------
		node.name: node_01
		node.master: true
		node.data: true
		#
		# Add custom attributes to the node:
		#
		#node.attr.rack: r1
		#
		# ----------------------------------- Paths ------------------------------------
		#
		# Path to directory where to store the data (separate multiple locations by comma):
		#
		path.data: /var/data/elasticsearch
		#
		# Path to log files:
		#
		path.logs: /var/log/elasticsearch
		network.host: 0.0.0.0
		http.port: 9200
		transport.tcp.port: 9300

		discovery.zen.ping.unicast.hosts: ["192.168.52.131:9300","192.168.52.132:9300", "192.168.52.133:9300"]

		discovery.zen.minimum_master_nodes: 2 
		192.168.52.132配置信息：

		只需要修改node.name即可：

		node.name: node_02

		192.168.52.133配置信息：

		node.name: node_03

		分别启动三台服务器上的es，（这里不能使用root用户启动，请创建个普通用户，如何创建，略去）

		查看集群信息：

		浏览器访问：http://192.168.52.131:9200/_cat/nodes?v

		显示结果如下：

		ip             heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
		192.168.52.132           13          94   6    0.11    0.26     0.23 mdi       -      node_02
		192.168.52.133           14          93   4    0.10    0.17     0.14 mdi       *      node_03
		192.168.52.131           13          92  15    0.22    0.50     0.28 mdi       -      node_01
		到此搭建成功了。。
	11.安装go
		下载go包并安装
		配置环境变量
			vi /etc/profile, 在export后加bin路径
			重启环境变量 source /etc/profile
	12.降级(降级主要针对不是特别重要但是比较占用资源的模块)
		1.降级的种类
			根据开关的位置分为：代码降级和前置降级
			读写降级：读降级和写降级
				读降级：数据从MySQL->缓存(redis)->静态资源->兜底文件(就是默认)->nginx返回(直接返回空)
			(nginx的限流:ngx_http_limit_req_module,ngx_http_limit_conn_module模块)
		2.手动降级,自动降级
		操作：
			1.nginx加载lua模块
			2.lua文件从redis读取开关设置来判断是从哪里获取数据(微服务/缓存/静态文件/...)
		3.漏桶原理(基本都是nginx实现),还有多(nginx+lua实现)
			需要依赖lua-resty-limit-traffic模块(是openresty的一个扩展)
			
			-- 加载nginx—lua限流模块
			local limit_req = require "resty.limit.req"
			-- 因为模块中控制粒度为毫秒级别，所以可以做到毫秒级别的平滑处理
			local lim, err = limit_req.new("my_limit_req_store", 50, 1000) #这里只是一个计算值,这里设置rate=50个请求/每秒，漏桶桶容量设置为1000个请求
			if not lim then
			  ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
			  return ngx.exit(501)
			end
			local key = ngx.var.binary_remote_addr
			local delay, err = lim:incoming(key, true)
			ngx.say(delay)
			if ( delay <0 or delay==nil ) then
			  return ngx.exit(502)
			end
			-- delay值就是当前这个请求的等待时长，这个时长是通过resty.limit.req模块计算出来的
			-- 1000以外的就溢出
			if not delay then
			  if err == "rejected" then
			    return ngx.say("1000以外的就溢出")
			    -- return ngx.exit(502)
			  end
			  ngx.log(ngx.ERR, "failed to limit req: ", err)
			  return ngx.exit(502)
			-- 加载nginx—lua限流模块
			local limit_req = require "resty.limit.req"
			-- 这里设置rate=50个请求/每秒，漏桶桶容量设置为1000个请求
			-- 因为模块中控制粒度为毫秒级别，所以可以做到毫秒级别的平滑处理
			local lim, err = limit_req.new("my_limit_req_store", 50, 1000)
			if not lim then
			  ngx.log(ngx.ERR, "failed to instantiate a resty.limit.req object: ", err)
			  return ngx.exit(501)
			end
			local key = ngx.var.binary_remote_addr
			local delay, err = lim:incoming(key, true)
			ngx.say(delay)
			if ( delay <0 or delay==nil ) then
			  return ngx.exit(502)
			end
			-- delay值就是当前这个请求的等待时长，这个时长是通过resty.limit.req模块计算出来的
			-- 1000以外的就溢出
			if not delay then
			  if err == "rejected" then
			    return ngx.say("1000以外的就溢出")
			    -- return ngx.exit(502)
			  end
			  ngx.log(ngx.ERR, "failed to limit req: ", err)
			  return ngx.exit(502)
			end
			-- 50-100的等待从微服务+mysql获取实时数据;（100-50）/50 =1
			if ( delay >0 and delay <=1 ) then
			  ngx.sleep(delay)
			-- 100-400的直接从redis获取实时性略差的数据;（400-50）/50 =7
			elseif ( delay >1 and delay <=7 ) then
			  local resp, err = redis_instance:get("redis_goods_list_advert")
			  ngx.say(resp)
			  return
			-- 400-1000的从静态文件获取实时性非常低的数据（1000-50）/50 =19
			elseif ( delay >7) then
			  ngx.header.content_type="application/x-javascript;charset=utf-8"
			  local file = "/etc/nginx/html/goods_list_advert.json"
			  local f = io.open(file, "rb")
			  local content = f:read("*all")
			  f:close()
			  ngx.print(content)
			  return
			end
			ngx.say("进入查询微服务+mysql") #实际中,这一步就是正常请求微服务处理数据
		4.限制每个IP的请求速度(nginx中limit_req_zone),需要下载nginx官方限流模块
		limit_req_zone $binary_remote_addr zone=one:10m rate=60r/m;
			server {
					...
						location /login.html {
						limit_req zone=one;
					...
				}
			}
		5.CDN域名解析
			<span class="image featured"><img src="{{ 'assets/images/other/CDNCname.jpg' | relative_url }}" alt="" /></span>
			主要通过squid来实现,squid(的端口3128)的配置文件(squid.conf)修改原站的静态资源地址
				http_port 3128 accel vhost vport // 主要是这两行
				cache peer 192.168.232.204 parent 80 0 originserver #这个是cdn服务器上的squid配置源服务器地址
				(用户->源站地址->通过阿里Cname配置到CDN服务器->CDN服务的Nginx反向代理到squid->squid配置源服务器地址->加载静态文件)
				(如果源服务器关闭,squid有缓存,squid的配置文件是可以设置缓存目录的,也可以设置缓存过期时间,还是可以访问静态文件的)
				<span class="image featured"><img src="{{ 'assets/images/other/CDNconfig.jpg' | relative_url }}" alt="" /></span>
		6.布隆过滤器(bloomfilter)
			(redis加载扩展模块,是通过下载扩展,然后在配置文件中loadmodule加载)
			首先将位数组进行初始化,将里面每个位都设置位0.对于集合里面的每一个元素,将元素依次通过3个哈希函数进行映射,每次映射都会产生一个哈希值,这个值对应位数组上面的一个点,然后将位数组对应的位置标记为1.查询W元素是否存在集合中的时候,同样的方法将W通过哈希映射到位数组上的3个点.如果3个点的其中有一个点不为1,则可以判断该元素一定不存在集合中.反之,如果3个点都为1,则该元素可能存在集合中.注意:此处不能判断该元素是否一定存在集合中,可能存在一定的误判率.可以从图中可以看到:假设某个元素通过映射对应下标为4,5,6这3个点.虽然这3个点都为1,但是很明显这3个点是不同元素经过哈希得到的位置,因此这种情况说明元素虽然不在集合中,也可能对应的都是1,这是误判率存在的原因.

			loadmodule /usr/local/redis/redisbloom-1.1.1/rebloom.so #INITIAL_SIZE 800000 ERROR_RATE 0.1
			#位向量长度100M，误差率千分之一（百分之0.1）
			[root@redis]# redis-server redis.conf
			主要命令有
			bf.add 添加元素
			bf.exists 查询元素是否存在
			bf.madd 一次添加多个元素
			在 redis 中有两个值决定布隆过滤器的准确率：
			error_rate：允许布隆过滤器的错误率，这个值越低过滤器的位数组的大小越大，占用空间也就越大。
			initial_size：布隆过滤器可以储存的元素个数，当实际存储的元素个数超过这个值之后，过滤器的准确率会下降。
			redis 中有一个命令可以来设置这两个值：
			示例：
			php-redis扩展中有个函数可以调用原始的redis指令：
			添加：在向redis set值的之后，调用bf.add添加到过滤器。 检查：在穿过了redis去到Mysql之前，调用bf.exists检查
			一下，如果不存在。
			布隆过滤器可以用在查询和写入分开的业务模式下，一个业务会把key写入redis和BF，另一个业务来搜索查找这个
			key。 如果get和set在同一个地方就不能用BF啦。
			6 解决本节刚开始提出的缓存穿透问题
			这里我们按着本节5.2的方式来实现，实现起来相对简单些
			bf.reserve test 0.1 100000000
			第一个值是过滤器的名字。
			第二个值为 error_rate 的值。
			第三个值为 initial_size 的值。
			注意必须在add之前使用bf.reserve指令显式创建，如果对应的 key 已经存在，bf.reserve会报错。同时设置的错误率
			越低，需要的空间越大。如果不使用 bf.reserve，默认的error_rate是 0.01，默认的initial_size是 100。
			#新建一个过滤器
			bf.reserve test 0.1 100000000  # test是布隆过滤器名称，0.1是误判率，100000000是位向量长度
			#向过滤器中添加元素
			127.0.0.1:6379> bf.add test abc123 #test是布隆过滤器名称，abc123是需要判断的元素
			#判断元素是否在过滤器中
			127.0.0.1:6379> bf.exists test abc123 #test是布隆过滤器名称，abc123是需要判断的元素
			1
			// php实现布隆过滤器
			$redis = new \Redis();
			$redis->connect('127.0.0.1', 6379);
			//先看看布隆过滤器中验证数据是否存在， 如果布隆过滤器说没有，那就肯定没有
			$re = $redis->rawCommand('bf.exists', 'goods_sku_code', '商品编码123456abcdefg');
			if($re == 0){
			  die("商品编码不存在");
			}else{
			  //这里写查询mysql代码
			}
			var_dump($re);
			<span class="image featured"><img src="{{ 'assets/images/other/redisBloomFilter.jpg' | relative_url }}" alt="" /></span>
			注:(如果使用PHP+redis实现布隆过滤器的话,主要使用setbit与getbit函数)
			(为什么使用两个hash函数,这样计算结果不同,减少误判率)
			<span class="image featured"><img src="{{ 'assets/images/other/RedisBloomFilterliucheng.jpg' | relative_url }}" alt="" /></span>
			使用场景:消耗内存过多的数据
				比如敏感词,身份证号等等
				1.Google著名的分布式数据库Bigtable以及Hbase使用了布隆过滤器来查找不存在的行或列以减少磁盘查找的ＩＯ
				2.检查垃圾邮件地址
				3.Google chrome 浏览器使用bloom filter识别恶意链接
				4.文档存储检索系统也采用布隆过滤器来检测先前存储的数据
				5.爬虫URL地址去重 A,B 两个文件
				6.解决缓存穿透问题:缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞)
		7.分库分表
			1.表数据过大导致查询问题
			2.降低压力,降低负载
			3.容灾
			映射关系,根据某个字段进行取模,根据映射关系确认哪个库
			逻辑:
				订单表进行hash分库分表
				再根据用户和商家进行分库分表(会出现数据冗余),这样针对店铺/用户查询就可在指定库指定表查询信息
			数据统计问题:
				1.使用ES进行统计
				2.使用统计库(包含各种统计数据,相当于给统计单独拉出来一个库,使用Rabbitmq进行累加)
			热点数据单独处理(比如较大数据量的商家单独使用数据库,相当于大客户优先)
</pre>