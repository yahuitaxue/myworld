---
title: ES初识
author: Yahui
layout: sql
category: SQL
---

书名：《-》

<pre style="text-align: left;">
	提示:
		默认端口9200
	1.核心
		1.搜索,聚合分析,大数据存储
		2.分布式,高性能,高可用,易扩展,易维护
		3.支持文本搜索,结构化数据,非结构化数据,地理位置搜索等(全文检索只是全球众多公司利用ES解决各种挑战的一小部分)
	2.下载
		1.安装Java
		2.安装ES(https://www.elastic.co/cn/downloads/elasticsearch)
			解压后,直接执行bin/elasticsearch.bat即可
			访问http://localhost:9200/
		3.安装kibana(ES的客户端工具https://www.elastic.co/cn/downloads/kibana)
			解压后,直接执行bin/kibana.bat即可
			访问http://localhost:5601
			配置ES服务地址:kibana
			(可以配置中文插件i18n.locale: "zh-CN")
		4.安装Head插件(界面插件git://github.com/mobz/elasticsearch-head.git)
			下载并配置
				// README.textile内容
				git clone git://github.com/mobz/elasticsearch-head.git
				cd elasticsearch-head
				npm install
				npm run start
				open http://localhost:9100
			ES的配置文件(elasticsearch.yml)
				// 如果需要修改服务地址
				network.host: 192.168.0.1
				cluster.initial_master_nodes: ["node-1", "node-2"]
				// 如果访问跨域报错
				http.cors.enabled: true
				http.cors.allow-origin: "*"
		5.安装IK中文分词器插件(https://github.com/medcl/elasticsearch-analysis-ik/releases)
			下载后直接放入ES的插件目录中即可(不是下载的源码,而是jar包)
	3.理解
		1.ES是面向文档,关系型数据库进行对比
			数据库 -> 索引(indices)
			表 -> types(逐渐就被废弃掉)
			行 -> document
			字段 -> fields
		2.一个分片是一个Lucene索引, 一个包含倒排索引的文件目录
		3.查看ik分词
			GET _analyze
			{
			  "analyzer": "ik_smart", // 最粗粒度,分词的时候只分一次,句子里面的每个字只会出现一次。
			  "text": ["中国人民好幸福"]
			}
			GET _analyze
			{
			  "analyzer": "ik_max_word", // 最大粒度分词,只要在词库里面出现过的就拆分出来。如果没有出现的单字。如果已经在词里面出现过,那么这个就不会以单字的形势出现。
			  "text": ["中国人民好幸福"]
			}
			如果分词并不是想要的, 可以进行手动修改(注意,修改的时候编码必须为utf-8)
			1.在ik插件config目录中,可以看到很多dic的分词文档
			2.创建自定义dic文档
			3.想要认为是一个词的写入文档的一行保存(test.dic, 首行写入"八重神子")
			4.修改配置文件(IKAnalyzer.cfg.xml),将新增的dic文档添加进去
			5.重启ES("八重神子"就会被当做一个完整的词)
	4.操作(Rest风格)
		(同理,这样也是可以使用postman等工具发送请求)
		1.PUT
			声明:
				(增加,如果已经存在则会更新(注意的是,如果是更新,没有的字段则会被删除,所以一般还是处理新增操作))
			模式:
				PUT /index_name(索引名)/type_name(类型(未来就不用了,默认是_doc))/1(文档id)
				{
					(请求体)
				  "name":"heihei",
				  "age":10086
				}
			返回结果:
				{
					"_index" : "index_name",
					"_type" : "type_name",
					"_id" : "1",
					"_version" : 1, // 如果修改的话,版本号会增加
					"result" : "created",
					"_shards" : {
						"total" : 2,
						"successful" : 1,
						"failed" : 0
					},
					"_seq_no" : 0,
					"_primary_term" : 1
				}
			例:
				PUT /test2
				{
					"mappings" : { // 规则
						"properties" : { // 属性
							"name" :{
								"type" : "text"
							},
							"age" :{
								"type" : "long" // 如果类型是keyword,则不会被分词(可以用_analyze来查看)
							}
						}
					}
				}
		2.POST
			1./索引名/类型 (创建)
				POST /index_name/_doc/
				{
				    "name": "hahaha"
				}
			2./索引名/类型/文档id/_update (更新)
				POST /index_name/_doc/1/_update
				{
				  "doc": {
				    "name": "hahaha"
				  }
				}
			3./索引名/类型/_search (查询所有)
				POST /index_name/_doc/_search?q=name:haha
				{
					"took" : 0,
					"timed_out" : false,
					"_shards" : {
						"total" : 1,
						"successful" : 1,
						"skipped" : 0,
						"failed" : 0
					},
					"hits" : {
						"total" : {
						  "value" : 5,
						  "relation" : "eq"
						},
						"max_score" : 0.08701137, // 匹配度
						"hits" : [
							{
								"_index" : "index_name",
								"_type" : "_doc",
								"_id" : "1",
								"_score" : 0.08701137, // 匹配度
								"_source" : {
									"name" : "hahaha",
									"age" : 100089
								}
							},
							{...}
						]
					}
				}
		3.DELETE
			1./索引名称 (删除索引)
			2./索引名称/类型/文档id (删除文档)
		4.GET
			/索引名称/类型/文档id (根据ID查询)
			/索引名称(/类型,也可以不加)/_search (根据条件查询,与POST一样)
				GET /index_name/_doc/_search
				{
				  "query": {
				    "bool": { // 组合查询,过滤多个条件
				      "must": [ // 相当于and查询 -> "must_not"
				      "should": [ // 相当于or查询 -> "should_not"
				        {
				          "match": { // 会使用分词器进行查找,如果用term则是精确查找
				            "name": "hahaha",
				            "tag":"男 技术" // 多个条件查询,只用空格隔开即可
				          },
				          {
				            ...
				          }
				        }
				      ],
				      "filter":{
				        "terms":{
				          "age": [10, 50] // or查询
				        }
				      },
				      "filter":{
				        "range":{ // 区间查找
				          "age":{
				            "gte":10,
				            "let":50
				          }
				        }
				      }
				    }
				  },
				  "_source": [
				    "name"
				  ],
				  "sort": [
				    {
				      "age": {
				        "order": "desc"
				      }
				    }
				  ],
				  "highlight":{ // 高亮查询
				    "pre_tags":"<p class='key' style 'color:red'>", // 自定义标签
				    "post_tags":"</p>",
				      "fields": {
				        "name": {}
				      }
				  },
				  "from": 0, // 分页查询
				  "size": 2
				}
	集群搭建
	1.下载ES,复制多个
	2.配置文件修改
		cluster.name: my-els                               # 集群名称
		node.name: els-node1                               # 节点名称，仅仅是描述名称，用于在日志中区分

		path.data: /opt/elasticsearch/data                 # 数据的默认存放路径
		path.logs: /opt/elasticsearch/log                  # 日志的默认存放路径

		network.host: 192.168.60.201                       # 当前节点的IP地址
		http.port: 9200                                    # 对外提供服务的端口，9300为集群服务的端口

		#添加如下内容
		#culster transport port
		transport.tcp.port: 9300
		transport.tcp.compress: true

		discovery.zen.ping.unicast.hosts: ["192.168.60.201", "192.168.60.202","192.168.60.203"]       
		# 集群个节点IP地址，也可以使用els、els.shuaiguoxia.com等名称，需要各节点能够解析

		discovery.zen.minimum_master_nodes: 2              # 为了避免脑裂，集群节点数最少为 半数+1
	默认情况下一个索引创建 5 个主分片，每个主分片会有一个从分片 (5 primary + 5 replica = 10 个分片)
	3.倒排索引原理
		term:
			在ES中，关键词被称为term。
		postings list:
			文档列表，作为文档的唯一标识的，ES 会对这些存入的文档进行处理，转化成一个唯一的整型 id，每个文档被分配一个唯一的 id，从0到(2^31)-1。
		term dictionary:
			如何快速的在海量 term 中查询到需要的 term 呢？遍历显然是不够的，于是乎就有了term dictionary，ES 为了能快速查找到 term，将所有的 term 排了一个序，二分法查找。是不是感觉有点眼熟，这不就是 MySQL 的索引方式的，直接用 B+树建立索引词典指向被索引的数据。
		term index:
			那问题又来了，Term Dictionary 应该放在哪里？肯定是放在内存里面吧？磁盘 io 那么慢。就像 MySQL 索引就是存在内存里面了。重点是 ES 默认可是会对全部 字段进行索引，必然会消耗巨大的内存，此时还能放内存吗？内存会爆。于是乎就有了term index 从数据结构上分类算是一个“Trie 树”，也就是我们常说的字典树（这是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题）。这棵树不会包含所有的 term，它包含的是 term 的一些前缀（这也是字典树的使用场景，公共前缀）。通过 term index 可以快速地定位到 term dictionary 的某个 offset，然后从这个位置再往后顺序查找。是不是像查字典，先查偏旁，在到这个偏旁的所有字，再到具体的字。
		Cluster
			代表一个集群，集群中有多个节点，其中有一个为主节点，这个主节点是可以通过选举产生的，主从节点是对于集群内部来说的。es的一个概念就是去中心化，字面上理解就是无中心节点，这是对于集群外部来说的，因为从外部来看es集群，在逻辑上是个整体，你与任何一个节点的通信和与整个es集群通信是等价的。
		Shards
			代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引拆分成多个，分布到不同的节点上。构成分布式搜索。分片的数量只能在索引创建前指定，并且索引创建后不能更改。
		replicas
			代表索引副本，es可以设置多个索引的副本，副本的作用一是提高系统的容错性，当某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负载均衡。
		Recovery
			代表数据恢复或叫数据重新分布，es在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。
		<span class="image featured"><img src="{{ 'assets/images/other/esBase.jpg' | relative_url }}" alt="" /></span>
		1、每个索引会被分成多个分片shards进行存储，默认创建索引是分配5个分片进行存储。每个分片都会分布式部署在多个不同的节点上进行部署，该分片成为primary shards。
			注意：索引的主分片primary shards定义好后，后面不能做修改。
	　　2、为了实现高可用数据的高可用，主分片可以有对应的备分片replics shards，replic shards分片承载了负责容错、以及请求的负载均衡。
			注意: 每一个主分片为了实现高可用，都会有自己对应的备分片，主分片对应的备分片不能存放同一台服务器上。主分片primary shards可以和其他replics shards存放在同一个node节点上。
	　　3、documnet routing（数据路由）
	 　　　　当客户端发起创建document的时候，es需要确定这个document放在该index哪个shard上。这个过程就是数据路由。
	 　　　　路由算法：shard = hash(routing) % number_of_primary_shards
	 　　　　如果number_of_primary_shards在查询的时候取余发生的变化，无法获取到该数据
	 　　　　注意：索引的主分片数量定义好后，不能被修改
注
	rollover
		一般而言，客户端将数据每天写入一个索引，比如直接写入YYYY-MM-HH格式的索引，那么我们只需要在写入的客户端里面获取时间，然后得到相应的格式即可，例如logstash写入ES时索引名就是logstash-YYYY-MM-HH，但是实际使用中，我们总会遇到这种格式的索引名不太适用的情况（比如每天数据量很少，但是又需要保存很久的数据，或者每天数据量极大，每天一个索引已经不能容纳了），这个时候我们就需要考虑一个机制，将索引rollover，让我们能够按照具体情况调节写的索引。
		例:
			结合alias，我们可以实现客户端写alias，在需要时将alias指向一个新的索引，就可以自由地控制数据的写入了。
			1. 先create一个新的索引
				PUT test_index2
			2. 再将alias指向新的索引并移除旧的alias
			POST _aliases
			{
				"actions" : [
					{
						"remove" : {
							"index" : "test_index",
							"alias" : "test_alias",
						}
					},
					{
						"add" : {
							"index" : "test_index2",
							"alias" : "test_alias",
							"is_write_index" : true
						}
					}
				]
			}
			上文中，我们手动Rollover了一个索引，在运行过程中，我们需要不断的获取ES中索引的情况，然后判断是否进行Rollover。这里，我们可以用ES自带的Rollover接口替代，假设已经存在一个test_index, 和一个test_alias指向test_index。
			test_alias/_rollover/test_index2
			{
				"conditions": {
					"max_age": "7d",
					"max_docs": 1
				}
			}
	force_merge
		合并与 Lucene 索引在每个分片中保存的分段数有关。 强制合并操作允许通过合并来减少分段的数量。
		删除
			删除索引是会立即释放空间的，不存在所谓的“标记”逻辑。
			删除文档的时候，是将新文档写入，同时将旧文档标记为已删除。 磁盘空间是否释放取决于新旧文档是否在同一个segment file里面，因此ES后台的segment merge在合并segment file的过程中有可能触发旧文档的物理删除。
		更新
			每个段中维护一个.del 文件，ES 只是逻辑删除文档，在.del 文件中标记为已删除，查询依然可以查到，但是会在结果中过滤掉。
			引入版本的概念，旧版本的记录将在 .del 文件中标记为删除，新版本的文档将被索引到一个新段（Segment）。
			es在Segment数量达到一定得临界点会自动forcemerge。实际得生产情况，会极大得影响查询得效率。
		单索引
			# 语法 max_num_segments=1参数设置1,${index_name}填写要force_merge的index
			http://xxx.xxx.xxx.xxx:9200/${index_name}/_forcemerge?max_num_segments=1
		多索引
			# 语法 max_num_segments=1参数设置1,${index_name}填写要force_merge的index
			http://xxx.xxx.xxx.xxx:9200/${index_name1}, /${index_name2}/_forcemerge?max_num_segments=1
		# 全部
			http://xxx.xxx.xxx.xxx:9200/_forcemerge?max_num_segments=1
		参数说明
			max_num_segments：合并到的段数。要完全合并索引，请将其设置为1。默认值是简单地检查是否需要执行合并，如果需要，则执行合并。
			only_expunge_deletes：合并过程应该只删除其中有删除的段。在Lucene中，不会从段中删除文档，而只是将其标记为已删除。在段的合并过程中，将创建一个没有这些删除的新段。此标志仅允许合并具有删除的段。默认为false。请注意，这不会超过 index.merge.policy.expunge_deletes_allowed阈值。
			flush：强制合并后是否应该执行刷新。默认为 true。
</pre>